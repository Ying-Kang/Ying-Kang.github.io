<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>机器学习算法总结 - Koin's Blog</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u603b\u7ed3";
    var mkdocs_page_input_path = "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u603b\u7ed3.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Koin's Blog</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../about/">About</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">机器学习算法总结</a>
    <ul class="current">
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Koin's Blog</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>机器学习算法总结</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="1lr">1.LR</h1>
<ul>
<li></li>
<li>
<p><strong>目标函数构建</strong></p>
</li>
</ul>
<p>sigmoid函数表达式：
  $$
  g(z)=\frac{1}{1+e^{-z}}
  $$
  其导数是：
  $$
  g^{'}(z)=g(z)(1-g(z))
  $$
  因为二分类问题服从伯努利分布</p>
<p>所以令目标函数是：
  $$
  h_{\theta}(x)=\frac{1}{1+e^{- \theta x}}
  $$</p>
<p>$$
  p(y|x;\theta)=
  \left{
      \begin{aligned}
      1-h_{\theta}(x),  &amp;&amp;y=0\
      h_{\theta}(x),    &amp;&amp;y=1\
      \end{aligned}
  \right.
  $$</p>
<ul>
<li><strong>构建似然函数求解</strong></li>
</ul>
<p><strong><font color=#DC143C>为什么用似然函数？</font></strong></p>
<p><strong>因为最小二乘法求解是非凸，而最大似然函数可导凸函数，可以引入参数优化解决</strong></p>
<p>似然函数定义为：
  $$
  L(\theta)=\prod\limits_{1}^{N}{h_{\theta}(x)}^{y_i}(1-h_{\theta}(x))^{1-y_i}
  $$
  定义损失函数为：
  $$
  J=-lnL(\theta)=\sum{-y_{i}ln(h_{\theta}(x_{i}))}-(1-y_i)ln(1-h_{\theta}(x_{i}))
  $$
  <strong><font color=#DC143C>这个损失函数就是交叉熵损失函数</font></strong></p>
<p>其导数为：
  $$
  \frac{\partial J}{\partial\theta}=
  \frac{\partial J}{\partial h_{\theta}}*
  \frac{{\partial h_{\theta}}}{{\partial\theta}}=(h_{\theta}(x)-y_i)x_i
  $$
  进而推导出参数更新的公式为：
  $$
  \theta_j = \theta_i + \alpha\sum(h_{\theta}(x)-y_i)x_i
  $$</p>
<h1 id="2-svm">2. SVM</h1>
<ul>
<li>
<p><strong>假设超平面</strong>
  $$
  f(x)=\omega^{T}x+b=0
  $$</p>
</li>
<li>
<p><strong>正确分类需要满足的条件</strong></p>
</li>
</ul>
<p>正确分类需要满足
  $$
  \left{
  \begin{aligned}
  \omega^{T}x+b \geq+1,&amp;&amp;y_i=+1\
  \omega^{T}x+b \leq-1,&amp;&amp;y_i=-1
  \end{aligned}
  \right.
  $$
  上面两个式子相减得到：
  $$
  r=\frac{2}{||\omega||}
  $$
  此处的r就是函数间隔</p>
<ul>
<li><strong>满足函数间隔最大化</strong></li>
</ul>
<p>函数间隔需要最大，但是又不能分得最大，</p>
<p>这个要求使得构建的超平面能够将样本分开，但是又不至于分得太开过拟合
  $$
  max(r)\Rightarrow
  max(\frac{2}{||\omega||})\Rightarrow
  min\frac{1}{2}||\omega||^{2}\qquad
  s.t. y_i(\omega^{T}x+b)\geq1
  $$</p>
<ul>
<li><strong>带约束的优化问题（使用拉格朗日对偶函数求解）</strong>
  $$
  L(\omega,b,\alpha)=\frac{1}{2}||\omega||^{2}+\sum\alpha(1-y_i(\omega^{T}x+b))
  $$
  分别对w和a求导，可以得到：
  $$
  \left{
  \begin{aligned}
  \frac{\partial L}{\partial \omega}=0 \
  \frac{\partial L}{\partial b}=0
  \end{aligned}
  \right.
  \Rightarrow
  \left{
  \begin{aligned}
  \omega = \sum\alpha y x\
  \sum \alpha y_i=0
  \end{aligned}
  \right.
  $$
  代入原来的式子有
  $$
  L(\omega,b,\alpha)=\sum\alpha_{i} - \frac{1}{2}\sum\sum\alpha_{i}y_{i}\alpha_{j}y_{j}x^{T}<em j="j">{i}x</em>
  $$
  <strong><font color=#DC143C>为什么用拉格朗日对偶函数求解？</font></strong></li>
</ul>
<p><strong>1.合并成为一个表达式之后计算相对简单</strong></p>
<p><strong>2.引入参数之后转换为参数优化的问题</strong></p>
<p><strong>3.引入核函数，将数据映射到高维空间，可以解决一些低维线性不可分的问题</strong></p>
<p><strong>常见核函数有线性核，高斯核，拉普拉斯核</strong>，</p>
<ul>
<li><strong>构建模型</strong></li>
</ul>
<p>代入原来的超平面模型有：
  $$
  f(x)=\omega^{T}x+b=\sum\alpha y_{i}x^T_ix+b
  $$</p>
<p><strong>上面的推导属于硬函数间隔，软函数间隔就是在<span class="arithmatex">\(\frac{1}{2}w^2\)</span>的地方加上一个惩罚项（松弛变量）继续推导就行</strong></p>
<h1 id="3-knn">3. KNN</h1>
<p>kNN算法的核心思想是<strong>用距离最近的k个样本数据的分类来代表目标数据的分类</strong></p>
<p>例如，训练样本集中包含一系列数据，</p>
<p>这个数据包括样本空间位置（特征）和分类信息（即目标变量，属于红色三角形还是蓝色正方形），</p>
<p>要对中心的绿色数据的分类。运用<strong>kNN</strong>算法思想，距离最近的k个样本的分类来代表测试数据的分类，</p>
<p>那么：
当k=3时，距离最近的3个样本在实线内，具有<strong>2</strong>个红色三角和<strong>1</strong>个蓝色正方形<strong>，因此将它归为红色三角。
当k=5时，距离最近的5个样本在虚线内，具有</strong>2<strong>个红色三角和</strong>3<strong>个蓝色正方形</strong>，因此将它归为蓝色正方形。</p>
<p><img alt="img" src="https://img-blog.csdn.net/20170504140232146?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW94aWdhbmRhc2h1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" /></p>
<p>优点：</p>
<ul>
<li>
<p>监督学习：可以看到，kNN算法首先需要一个<strong>训练样本集</strong>，这个集合中含有<strong>分类信息</strong>，因此它属于<strong>监督学习</strong></p>
</li>
<li>
<p>通过计算距离来衡量样本之间相似度，算法简单，易于理解和实现</p>
</li>
<li>对异常值不敏感</li>
</ul>
<p>一般情况下，kNN有如下流程：</p>
<p>（1）收集数据</p>
<p>（2）计算到每个训练样本数据的距离；</p>
<p>（3）按照距离递增的顺序排序；</p>
<p>（4）选取距离最近的<strong>k</strong>个点；</p>
<p>（5）确定这k个点中分类信息的频率；</p>
<p>（6）返回前k个点中出现频率最高的分类，作为当前测试数据的分类。</p>
<h1 id="4k-means">4.K-Means</h1>
<p>随机选取k个点和随机初始化聚类中心</p>
<p>遍历所有点计算该点到那k个聚类中心点的距离</p>
<p>重新计算聚类中心</p>
<p>重复上述步骤直到达到终止条件，比如迭代次数</p>
<pre><code class="language-python">import numpy as np


def k_means(x, k, epoch):
    clusters = []
    centers = []
    for i in range(k):
        centers.append(x[i])
        clusters.append([])
    center = np.array(centers)

    for _ in range(epoch):
        for i in range(k):
            clusters[i] = []
        # 分配每一个点到距离最近的聚类中心
        for i in range(x.shape[0]):
            distances = np.sum((center - x[i]) ** 2, axis=1)
            clusters[int(np.argmin(distances))].append(i)
        # 根据聚簇，重新计算聚类中心
        for i in range(k):
            center[i] = np.sum(x[clusters[i]], axis=0) / len(clusters[i])

</code></pre>
<h1 id="5decision-tree">5.Decision Tree</h1>
<p>决策树是一种机器学习的方法。</p>
<p>决策树的生成算法有ID3, C4.5等。</p>
<p>决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，</p>
<p>每个分支代表一个判断结果的输出，</p>
<p>最后每个叶节点代表一种分类结果。</p>
<p><strong>因为每一个非叶子节点都是一轮判断，所以用什么作为判断依据至关重要</strong></p>
<p>也因此衍生出了两种算法，分别是ID3和C4.5</p>
<ul>
<li>ID3：使用信息增益来作为判断的指标
  $$
  信息增益=信息熵-条件熵
  $$
  <strong>信息熵是代表随机变量的复杂度</strong></li>
</ul>
<p><strong>条件熵代表在某一个条件下，随机变量的复杂度</strong></p>
<p>信息增益准则其实是对可取值数目较多的属性有所偏好</p>
<p>所以假如某一类出现的数目特别少时，会特别准确构建一个分支去判断，这就容易导致过拟合</p>
<p>不具备泛化能力，于是引入了信息增益率，并应用在C4.5中</p>
<ul>
<li>C4.5：使用信息增益率来进行判断的指标：
  $$
  信息增益率=信息增益/IV(a)
  $$
  其中IVa是某一类属性的固有值</li>
</ul>
<p><strong>C4.5算法不直接选择增益率最大的候选划分属性，</strong></p>
<p><strong>候选划分属性中找出信息增益高于平均水平的属性（<em>这样保证了大部分好的的特征</em>），</strong></p>
<p><strong>再从中选择增益率最高的（又保证了不会出现编号特征这种极端的情况）</strong></p>
<ul>
<li>CART</li>
</ul>
<p>与ID3不同的是CART使用的是GINI指数来作为判断不纯度的指标</p>
<h1 id="6random-forest">6.Random Forest</h1>
<p>集成学习：通过建立几个模型组合的来解决单一预测问题。</p>
<p>它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。</p>
<p>这些预测最后结合成单预测，因此优于任何一个单分类的做出预测</p>
<p>随机森林中每一棵树都可以看做是一棵CART（分类回归树）</p>
<ul>
<li>bootstrap sample：随机且有放回地从训练集中的抽取N个训练样本</li>
</ul>
<p>如果不进行随机抽样，每棵树的训练集都一样，</p>
<p>那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；</p>
<ul>
<li>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，</li>
</ul>
<h1 id="7">7.如何解决过拟合</h1>
<ul>
<li>数据层面：获取更多数据</li>
<li>模型层面：网络结构改进，结合多种模型</li>
<li>训练层面：early stop，增加扰动，drop out</li>
</ul>
<h1 id="8dropout">8.dropout</h1>
<p>在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征</p>
<h1 id="9">9.正则化</h1>
<p>正则化是为了防止过拟合</p>
<p>正则化实际是在原始的损失函数上加上了一个惩罚项</p>
<p>假如移项过去的话就可以变成求两个图像切线的问题</p>
<p><strong>平方差项不是想拼命变小吗，我给你加一个函数，你越变小，我越变大</strong></p>
<ul>
<li>L1 正则化 ( Lasso 正则化)：<span class="arithmatex">\(h_\theta=\sum\limits_{j=1}^n{|\theta_j|}\)</span></li>
</ul>
<p>菱形和等高线的切线，因为这个切线接近于坐标轴，所以决定了l1正则化的稀疏性</p>
<ul>
<li>L2 正则化 ( Ridge 正则化)：<span class="arithmatex">\(h_\theta=\sum\limits_{j=1}^n{\theta_j^2}\)</span></li>
</ul>
<p>圆形和等高线的切线， L2 正则化的约束边界光滑且可导</p>
<h1 id="10batch-normalization-layer-normalization">10.batch normalization &amp; layer normalization</h1>
<p>数据进行训练之前常常白化，使得数据独立同分布</p>
<p>白化常用的是PCA，步骤如下：</p>
<ul>
<li>减均值后计算协方差矩阵</li>
<li>求特征值和特征向量，排序取前k作为特征向量</li>
</ul>
<p>BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况（Internal Covariate Shift）</p>
<p>具体步骤：</p>
<ul>
<li>
<p>均值方差归一化</p>
</li>
<li>
<p>尺度变换和偏移</p>
</li>
</ul>
<p>这一步是BN精髓，因为归一化后的<span class="arithmatex">\(x_{i}\)</span>基本会被限制在正态分布下，使得网络的表达能力下降</p>
<p>BN针对批量训练样本进行</p>
<p>LN 针对单个训练样本进行，不依赖于其他数据</p>
<h1 id="11softmax">11.softmax推导</h1>
<p>softmax的公式是：
$$
a_j^L=\frac{e^{z_j^L}}{\sum_k{e^{z_k^L}}}
$$
其中<span class="arithmatex">\(z_j^L\)</span>表示第L层（通常是最后一层）第j个神经元的输入</p>
<p><span class="arithmatex">\(a_j^L\)</span>表示第L层第j个神经元的输出</p>
<p>分母表示了第L层所有神经元的输入之和</p>
<p>反向传播在求导的时候，根据链式法则</p>
<p>其实最主要关注的就是a相对于z的偏导</p>
<p>其余项都是已知或者可以间接求出的</p>
<p>a相对于z的偏导分为两种情况：</p>
<div class="arithmatex">\[
\left\{
\begin{array}{l}
\frac{\partial a_j}{\partial z_i}=a_j*(1-a_i),&amp;&amp;i=j\\
\frac{\partial a_j}{\partial z_i}=-a_j*a_i,&amp;&amp;others
\end{array}
\right.
\]</div>
<p>正向和反向传播实例链接：https://zhuanlan.zhihu.com/p/24801814</p>
<p>​    </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../about/" class="btn btn-neutral" title="About"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../about/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../javascripts/config.js" defer></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
